# Day 4 (Agent Quality)

This whitepaper addresses the challenge of assuring quality in Al agents by introducing a holistic evaluation framework. The necessary technical foundation for this is Observability, built on three pillars: Logs (the diary), Traces (the narrative), and Metrics (the health report), enabling a continuous feedback loop using scalable methods like LLM-as-a-Judge and Human-in-the-Loop (HITL) evaluation.  
  
For today's codelabs, you'll learn how to use logs, traces, and metrics to get full visibility into your agent's decision-making process, allowing you to debug failures and understand why your agent behaves the way it does. In the second codelab, you'll learn how to evaluate your agents to score your agent's response quality and tool usage.  
  
## **Day 4 Assignment**  
  
Complete Unit 4 - “Agent Quality”:

  
a. [] Listen to the [summary podcast episode](https://www.youtube.com/watch?v=LFQRy-Ci-lk) for this unit, created by [NotebookLM](https://notebooklm.google.com/).  
 
b. [] To complement the podcast, read the [Agent Quality whitepaper](https://www.kaggle.com/whitepaper-agent-quality).  

c. [] Complete these codelabs on Kaggle:

    - [] [Implement](https://www.kaggle.com/code/kaggle5daysofai/day-4a-agent-observability) observability to help you debug your agents.  
 
    - [] [Evaluate](https://www.kaggle.com/code/kaggle5daysofai/day-4b-agent-evaluation) your agents.